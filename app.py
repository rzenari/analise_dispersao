# ==============================================================================
# 1. IMPORTA√á√ÉO DAS BIBLIOTECAS
# ==============================================================================
import streamlit as st
import pandas as pd
import geopandas as gpd
import numpy as np
from sklearn.cluster import DBSCAN
from scipy.spatial import distance
from math import sqrt
import folium
from streamlit_folium import st_folium
from folium.plugins import MarkerCluster, HeatMap
import h3
from shapely.geometry import Polygon

# ==============================================================================
# 2. CONFIGURA√á√ÉO DA P√ÅGINA E T√çTULOS
# ==============================================================================
st.set_page_config(layout="wide", page_title="An√°lise de Dispers√£o Geogr√°fica")

st.title("üó∫Ô∏è Ferramenta de An√°lise de Dispers√£o Geogr√°fica")
st.write("Fa√ßa o upload da sua planilha de cortes para analisar a distribui√ß√£o geogr√°fica e identificar clusters")

# ==============================================================================
# 3. FUN√á√ïES DE AN√ÅLISE (COM CACHE PARA PERFORMANCE)
# ==============================================================================

@st.cache_data
def carregar_dados_otimizado(arquivo_enviado):
    """L√™ o arquivo de forma otimizada, carregando apenas as colunas necess√°rias para a an√°lise."""
    colunas_necessarias = ['latitude', 'longitude', 'sucursal', 'centro_operativo', 'corte_recorte', 'prioridade', 'numero_ordem']
    arquivo_enviado.seek(0)
    
    def processar_dataframe(df):
        df.columns = df.columns.str.lower().str.strip()
        if not all(col in df.columns for col in colunas_necessarias):
            st.error("ERRO: Colunas essenciais (como latitude, longitude, numero_ordem, etc.) n√£o foram encontradas."); st.write("Colunas necess√°rias:", colunas_necessarias); st.write("Colunas encontradas:", df.columns.tolist())
            return None
        df['latitude'] = df['latitude'].astype(str).str.replace(',', '.').astype(float)
        df['longitude'] = df['longitude'].astype(str).str.replace(',', '.').astype(float)
        df = df.dropna(subset=['latitude', 'longitude'])
        return df

    try:
        df_csv = pd.read_csv(arquivo_enviado, encoding='utf-16', sep='\t', usecols=lambda c: c.strip().lower() in colunas_necessarias)
        st.success("Arquivo CSV lido com sucesso (codifica√ß√£o: utf-16)."); return processar_dataframe(df_csv)
    except Exception:
        try:
            arquivo_enviado.seek(0)
            df_csv_latin = pd.read_csv(arquivo_enviado, encoding='latin-1', sep=None, engine='python', usecols=lambda c: c.strip().lower() in colunas_necessarias)
            st.success("Arquivo CSV lido com sucesso (codifica√ß√£o: latin-1)."); return processar_dataframe(df_csv_latin)
        except Exception:
            try:
                arquivo_enviado.seek(0)
                df_excel_full = pd.read_excel(arquivo_enviado, engine='openpyxl')
                df_excel_full.columns = df_excel_full.columns.str.lower().str.strip()
                colunas_para_usar = [col for col in colunas_necessarias if col in df_excel_full.columns]
                df_excel = df_excel_full[colunas_para_usar]
                st.success("Arquivo Excel lido com sucesso."); return processar_dataframe(df_excel)
            except Exception as e:
                st.error(f"N√£o foi poss√≠vel ler o arquivo. √öltimo erro: {e}"); return None

@st.cache_data
def carregar_dados_completos(arquivo_enviado):
    """L√™ o arquivo completo com todas as colunas para a funcionalidade de download."""
    arquivo_enviado.seek(0)
    try:
        df = pd.read_csv(arquivo_enviado, encoding='utf-16', sep='\t')
        df.columns = df.columns.str.lower().str.strip()
        return df
    except Exception:
        try:
            arquivo_enviado.seek(0)
            df = pd.read_csv(arquivo_enviado, encoding='latin-1', sep=None, engine='python')
            df.columns = df.columns.str.lower().str.strip()
            return df
        except Exception:
            try:
                arquivo_enviado.seek(0)
                df = pd.read_excel(arquivo_enviado, engine='openpyxl')
                df.columns = df.columns.str.lower().str.strip()
                return df
            except Exception:
                return None

def calcular_nni_otimizado(gdf):
    """Calcula NNI de forma otimizada para mem√≥ria."""
    if len(gdf) < 3: return None, "Pontos insuficientes (< 3)."
    n_points = len(gdf)
    points = np.array([gdf.geometry.x, gdf.geometry.y]).T
    soma_dist_minimas = sum(distance.cdist([points[i]], np.delete(points, i, axis=0)).min() for i in range(n_points))
    observed_mean_dist = soma_dist_minimas / n_points
    try:
        total_bounds = gdf.total_bounds; area = (total_bounds[2] - total_bounds[0]) * (total_bounds[3] - total_bounds[1])
        if area == 0: return None, "√Årea inv√°lida."
        expected_mean_dist = 0.5 * sqrt(area / n_points)
        nni = observed_mean_dist / expected_mean_dist
        if nni < 1: interpretacao = f"Agrupado (NNI: {nni:.2f})"
        elif nni > 1: interpretacao = f"Disperso (NNI: {nni:.2f})"
        else: interpretacao = f"Aleat√≥rio (NNI: {nni:.2f})"
        return nni, interpretacao
    except Exception as e: return None, f"Erro no c√°lculo: {e}"

def executar_dbscan(gdf, eps_km=0.5, min_samples=3):
    """Executa o DBSCAN para encontrar clusters."""
    if gdf.empty or len(gdf) < min_samples: gdf['cluster'] = -1; return gdf
    raio_terra_km = 6371; eps_rad = eps_km / raio_terra_km
    coords = np.radians(gdf[['latitude', 'longitude']].values)
    db = DBSCAN(eps=eps_rad, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(coords)
    gdf['cluster'] = db.labels_
    return gdf

def gerar_resumo_didatico(nni_valor, n_clusters, percent_dispersos, is_media=False):
    """Gera um texto interpretativo considerando tanto o NNI quanto a % de dispers√£o."""
    if nni_valor is None: return ""
    
    prefixo = "Na m√©dia, o padr√£o" if is_media else "O padr√£o"

    if percent_dispersos > 50:
        titulo = "‚ö†Ô∏è **Padr√£o Misto (Agrupamentos Isolados)**"
        obs = f"Apesar da exist√™ncia de **{n_clusters} hotspots**, a maioria dos servi√ßos (**{percent_dispersos:.1f}%**) est√° **dispersa** pela regi√£o."
        acao = f"**A√ß√£o Recomendada:** Trate a opera√ß√£o de forma h√≠brida. Otimize rotas para os hotspots e agrupe os servi√ßos dispersos por setor ou dia para aumentar a efici√™ncia."
    elif nni_valor < 0.5:
        titulo = "üìà **Padr√£o Fortemente Agrupado (Excelente Oportunidade Log√≠stica)**"
        obs = f"{prefixo} dos cortes √© **fortemente concentrado** em √°reas espec√≠ficas, com poucos servi√ßos isolados."
        acao = f"**A√ß√£o Recomendada:** Crie rotas otimizadas para atender m√∫ltiplos chamados com baixo deslocamento. Avalie alocar equipes dedicadas para os **{n_clusters} hotspots** encontrados."
    elif 0.5 <= nni_valor < 0.8:
        titulo = "üìä **Padr√£o Moderadamente Agrupado (Potencial de Otimiza√ß√£o)**"
        obs = f"{prefixo} dos cortes apresenta **boa concentra√ß√£o**, indicando a forma√ß√£o de clusters."
        acao = f"**A√ß√£o Recomendada:** Identifique os **{n_clusters} hotspots** mais densos para priorizar o roteamento. H√° um bom potencial para agrupar servi√ßos e reduzir custos."
    elif 0.8 <= nni_valor <= 1.2:
        titulo = "üòê **Padr√£o Aleat√≥rio (Sem Padr√£o Claro)**"
        obs = f"{prefixo} dos cortes √© **aleat√≥rio**, sem concentra√ß√£o ou dispers√£o estatisticamente relevante."
        acao = f"**A√ß√£o Recomendada:** A log√≠stica para estes cortes tende a ser menos previs√≠vel. Considere uma abordagem de roteiriza√ß√£o di√°ria e din√¢mica."
    else: # nni_valor > 1.2
        titulo = "üìâ **Padr√£o Disperso (Desafio Log√≠stico)**"
        obs = f"{prefixo} dos cortes est√° **muito espalhado** pela √°rea de atua√ß√£o, com poucos ou nenhum hotspot."
        acao = f"**A√ß√£o Recomendada:** Planeje as rotas com anteced√™ncia para minimizar os custos de deslocamento. Considere agrupar atendimentos por setor em dias espec√≠ficos."

    return f"""
    <div style="background-color:#f0f2f6; padding: 15px; border-radius: 10px;">
    <h4 style="color:#31333f;">{titulo}</h4>
    <ul style="color:#31333f;">
        <li><b>Observa√ß√£o:</b> {obs}</li>
        <li><b>A√ß√£o Recomendada:</b> {acao}</li>
    </ul>
    </div>
    """

# ==============================================================================
# 4. L√ìGICA PRINCIPAL DA APLICA√á√ÉO
# ==============================================================================
st.sidebar.header("Controles")
uploaded_file = st.sidebar.file_uploader("Escolha a planilha de cortes", type=["csv", "xlsx", "xls"])

if uploaded_file is not None:
    df_analise = carregar_dados_otimizado(uploaded_file)
    df_original_completo = carregar_dados_completos(uploaded_file)

    if df_analise is not None and df_original_completo is not None:
        st.sidebar.success(f"{len(df_analise)} registros carregados!")
        st.sidebar.markdown("### Filtros da An√°lise")
        
        filtros = ['sucursal', 'centro_operativo', 'corte_recorte', 'prioridade']
        valores_selecionados = {}
        for coluna in filtros:
            if coluna in df_analise.columns:
                lista_unica = df_analise[coluna].dropna().unique().tolist()
                opcoes = sorted([str(item) for item in lista_unica])
                
                if coluna == 'prioridade':
                    valores_selecionados[coluna] = st.sidebar.multiselect(f"{coluna.replace('_', ' ').title()}", opcoes)
                else:
                    valores_selecionados[coluna] = st.sidebar.selectbox(f"{coluna.replace('_', ' ').title()}", ["Todos"] + opcoes)

        df_filtrado = df_analise.copy()
        for coluna, valor in valores_selecionados.items():
            if coluna in df_filtrado.columns:
                if coluna == 'prioridade':
                    if valor: df_filtrado = df_filtrado[df_filtrado[coluna].astype(str).isin(valor)]
                else:
                    if valor != "Todos": df_filtrado = df_filtrado[df_filtrado[coluna].astype(str) == valor]

        st.header("Resultados da An√°lise")
        
        if not df_filtrado.empty:
            st.sidebar.markdown("### Par√¢metros de An√°lise")
            eps_cluster_km = st.sidebar.slider("Raio do Cluster (km)", 0.1, 5.0, 1.0, 0.1, help="Define o raio de busca para agrupar pontos no DBSCAN.")
            min_samples_cluster = st.sidebar.slider("M√≠nimo de Pontos por Cluster", 2, 20, 5, 1, help="N√∫mero m√≠nimo de pontos para formar um hotspot.")
            hex_resolution = st.sidebar.slider("Resolu√ß√£o do Mapa Hexagonal", 5, 10, 8, 1, help="Define o tamanho dos hex√°gonos. N√∫meros maiores = hex√°gonos menores e mais detalhados.")

            gdf_com_clusters = executar_dbscan(
                gpd.GeoDataFrame(df_filtrado, geometry=gpd.points_from_xy(df_filtrado.longitude, df_filtrado.latitude), crs="EPSG:4326"),
                eps_km=eps_cluster_km, 
                min_samples=min_samples_cluster
            )
            
            st.sidebar.markdown("### üì• Downloads")
            ordens_agrupadas = gdf_com_clusters[gdf_com_clusters['cluster'] != -1]['numero_ordem']
            ordens_dispersas = gdf_com_clusters[gdf_com_clusters['cluster'] == -1]['numero_ordem']
            df_agrupados_download = df_original_completo[df_original_completo['numero_ordem'].isin(ordens_agrupadas)]
            df_dispersos_download = df_original_completo[df_original_completo['numero_ordem'].isin(ordens_dispersas)]
            csv_agrupados = df_agrupados_download.to_csv(index=False).encode('utf-8-sig')
            csv_dispersos = df_dispersos_download.to_csv(index=False).encode('utf-8-sig')
            st.sidebar.download_button(label="‚¨áÔ∏è Baixar Servi√ßos Agrupados", data=csv_agrupados, file_name='servicos_agrupados.csv', mime='text/csv', disabled=df_agrupados_download.empty)
            st.sidebar.download_button(label="‚¨áÔ∏è Baixar Servi√ßos Dispersos", data=csv_dispersos, file_name='servicos_dispersos.csv', mime='text/csv', disabled=df_dispersos_download.empty)

            tab1, tab2, tab3, tab4 = st.tabs(["üó∫Ô∏è An√°lise Geogr√°fica e Mapa", "üìä Resumo por Centro Operativo", "üî• Mapa Hexagonal de Densidade", "üí° Metodologia"])

            with tab1:
                col1, col2, col3 = st.columns(3)
                col1.metric("Total de Cortes Carregados", len(df_completo))
                col2.metric("Cortes na Sele√ß√£o Atual", len(df_filtrado))
                nni_valor_final = None; is_media_nni = False
                centros_operativos_selecionados = gdf_com_clusters['centro_operativo'].unique()
                if len(centros_operativos_selecionados) == 1 or len(gdf_com_clusters) < 5000:
                    nni_valor_final, nni_texto = calcular_nni_otimizado(gdf_com_clusters)
                else:
                    is_media_nni = True; resultados_nni = []; pesos = []
                    for co in centros_operativos_selecionados:
                        gdf_co = gdf_com_clusters[gdf_com_clusters['centro_operativo'] == co]
                        if len(gdf_co) > 10:
                            nni, texto = calcular_nni_otimizado(gdf_co)
                            if nni is not None: resultados_nni.append(nni); pesos.append(len(gdf_co))
                    if resultados_nni:
                        nni_valor_final = np.average(resultados_nni, weights=pesos)
                        if nni_valor_final < 1: nni_texto = f"Agrupado (M√©dia: {nni_valor_final:.2f})"
                        elif nni_valor_final > 1: nni_texto = f"Disperso (M√©dia: {nni_valor_final:.2f})"
                        else: nni_texto = f"Aleat√≥rio (M√©dia: {nni_valor_final:.2f})"
                    else: nni_texto = "Insuficiente para c√°lculo"
                help_nni = "O √çndice do Vizinho Mais Pr√≥ximo (NNI) mede se o padr√£o dos pontos √© agrupado, disperso ou aleat√≥rio. NNI < 1: Agrupado (pontos mais pr√≥ximos que o esperado). NNI ‚âà 1: Aleat√≥rio (sem padr√£o). NNI > 1: Disperso (pontos mais espalhados que o esperado)."
                col3.metric("Padr√£o de Dispers√£o (NNI)", nni_texto, help=help_nni)
                n_clusters_total = len(set(gdf_com_clusters['cluster'])) - (1 if -1 in gdf_com_clusters['cluster'] else 0)
                total_pontos = len(gdf_com_clusters)
                n_ruido = list(gdf_com_clusters['cluster']).count(-1)
                percent_dispersos = (n_ruido / total_pontos * 100) if total_pontos > 0 else 0
                with st.expander("üîç O que estes n√∫meros significam? Clique para ver a an√°lise", expanded=True):
                     resumo_html = gerar_resumo_didatico(nni_valor_final, n_clusters_total, percent_dispersos, is_media=is_media_nni)
                     st.markdown(resumo_html, unsafe_allow_html=True)
                st.subheader("Resumo da An√°lise de Cluster")
                n_agrupados = total_pontos - n_ruido
                if total_pontos > 0:
                    percent_agrupados = (n_agrupados / total_pontos) * 100
                    c1, c2, c3 = st.columns(3)
                    c1.metric("N¬∫ de Hotspots (Clusters)", f"{n_clusters_total}")
                    sub_c1, sub_c2 = st.columns(2)
                    sub_c1.metric("N¬∫ Agrupados", f"{n_agrupados}", help="Total de servi√ßos que fazem parte de um hotspot.")
                    sub_c1.metric("% Agrupados", f"{percent_agrupados:.1f}%")
                    sub_c2.metric("N¬∫ Dispersos", f"{n_ruido}", help="Total de servi√ßos isolados, que n√£o pertencem a nenhum hotspot.")
                    sub_c2.metric("% Dispersos", f"{percent_dispersos:.1f}%")
                st.subheader(f"Mapa Interativo de Hotspots")
                st.write("D√™ zoom no mapa para expandir os agrupamentos e ver os pontos individuais.")
                if not gdf_com_clusters.empty:
                    map_center = [gdf_com_clusters.latitude.mean(), gdf_com_clusters.longitude.mean()]
                    m = folium.Map(location=map_center, zoom_start=11)
                    marker_cluster = MarkerCluster().add_to(m)
                    for idx, row in gdf_com_clusters.iterrows():
                        popup_text = ""
                        for col in ['prioridade', 'centro_operativo', 'corte_recorte']:
                            if col in row: popup_text += f"{col.replace('_', ' ').title()}: {str(row[col])}<br>"
                        folium.Marker(location=[row['latitude'], row['longitude']], popup=popup_text).add_to(marker_cluster)
                    st_folium(m, use_container_width=True, height=700, returned_objects=[])

            with tab2:
                st.subheader("An√°lise de Cluster por Centro Operativo")
                resumo_co = gdf_com_clusters.groupby('centro_operativo').apply(lambda x: pd.Series({
                    'Total de Servi√ßos': len(x),
                    'N¬∫ de Clusters': x[x['cluster'] != -1]['cluster'].nunique(),
                    'N¬∫ Agrupados': len(x[x['cluster'] != -1]),
                    'N¬∫ Dispersos': len(x[x['cluster'] == -1])
                }), include_groups=False).reset_index()
                resumo_co['% Agrupados'] = (resumo_co['N¬∫ Agrupados'] / resumo_co['Total de Servi√ßos'] * 100).round(1)
                resumo_co['% Dispersos'] = (resumo_co['N¬∫ Dispersos'] / resumo_co['Total de Servi√ßos'] * 100).round(1)
                resumo_co = resumo_co[['centro_operativo', 'Total de Servi√ßos', 'N¬∫ de Clusters', 'N¬∫ Agrupados', '% Agrupados', 'N¬∫ Dispersos', '% Dispersos']]
                st.dataframe(resumo_co, use_container_width=True)
            
            with tab3:
                st.subheader("Mapa Hexagonal de Densidade")
                st.write("Visualize a densidade de servi√ßos em √°reas geogr√°ficas fixas. A cor de cada hex√°gono representa o n√∫mero de cortes em seu interior. Esta vis√£o √© est√°vel e n√£o muda com o zoom.")
                limite_hexbin = 25000
                if len(df_filtrado) <= limite_hexbin:
                    df_filtrado['hex_id'] = df_filtrado.apply(lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], hex_resolution), axis=1)
                    df_hex = df_filtrado.groupby('hex_id').size().reset_index(name='contagem')
                    def hex_to_polygon(hex_id):
                        points = [(lon, lat) for lat, lon in h3.cell_to_boundary(hex_id)]
                        return Polygon(points)
                    df_hex['geometry'] = df_hex['hex_id'].apply(hex_to_polygon)
                    gdf_hex = gpd.GeoDataFrame(df_hex, crs="EPSG:4326")
                    map_center_hex = [df_filtrado.latitude.mean(), df_filtrado.longitude.mean()]
                    m_hex = folium.Map(location=map_center_hex, zoom_start=11)
                    folium.Choropleth(
                        geo_data=gdf_hex.to_json(), data=df_hex, columns=['hex_id', 'contagem'],
                        key_on='feature.properties.hex_id', fill_color='YlOrRd', fill_opacity=0.7,
                        line_opacity=0.2, legend_name='Contagem de Servi√ßos por Hex√°gono'
                    ).add_to(m_hex)
                    st_folium(m_hex, use_container_width=True, height=700, returned_objects=[])
                else:
                    st.info(f"O mapa hexagonal est√° desabilitado para sele√ß√µes com mais de {limite_hexbin:,.0f} pontos para garantir a performance. Por favor, aplique mais filtros para visualizar.".replace(",", "."))

            with tab4:
                st.subheader("As Metodologias por Tr√°s da An√°lise")
                st.markdown("""
                Para garantir uma an√°lise precisa e confi√°vel, utilizamos duas t√©cnicas complementares da estat√≠stica espacial:
                
                #### 1. Clustering Baseado em Densidade (DBSCAN)
                **O que √©?** DBSCAN (Density-Based Spatial Clustering of Applications with Noise) √© um algoritmo de machine learning que identifica agrupamentos de pontos em um espa√ßo. Ele √© a base da nossa contagem de "hotspots".
                
                **Como funciona?** O algoritmo define um "cluster" (ou hotspot) como uma √°rea onde existem muitos pontos pr√≥ximos uns dos outros. Ele agrupa esses pontos e, crucialmente, identifica os pontos que est√£o isolados em √°reas de baixa densidade, classificando-os como "dispersos" (ou "ru√≠do"). √â a partir desta an√°lise que calculamos o N¬∫ de Hotspots, o % de Servi√ßos Agrupados e o % de Servi√ßos Dispersos.
                
                #### 2. An√°lise do Vizinho Mais Pr√≥ximo (NNI)
                **O que √©?** O NNI (Nearest Neighbor Index) √© um √≠ndice estat√≠stico que responde a uma pergunta fundamental: "A distribui√ß√£o dos meus pontos √© agrupada, aleat√≥ria ou dispersa?" Ele √© a base da nossa m√©trica Padr√£o de Dispers√£o.
                
                **Como funciona?** A an√°lise mede a dist√¢ncia m√©dia entre cada servi√ßo e seu vizinho mais pr√≥ximo. Em seguida, compara essa m√©dia com a dist√¢ncia que seria esperada se os mesmos servi√ßos estivessem distribu√≠dos de forma perfeitamente aleat√≥ria na mesma √°rea geogr√°fica. O resultado √© um √≠ndice √∫nico:
                - **NNI < 1 (Agrupado):** Os servi√ßos est√£o, em m√©dia, mais pr√≥ximos uns dos outros do que o esperado pelo acaso.
                - **NNI ‚âà 1 (Aleat√≥rio):** N√£o h√° um padr√£o de distribui√ß√£o estatisticamente relevante.
                - **NNI > 1 (Disperso):** Os servi√ßos est√£o, em m√©dia, mais espalhados uns dos outros do que o esperado pelo acaso.
                
                Juntas, essas duas t√©cnicas fornecem uma vis√£o completa: o DBSCAN **encontra e conta** os agrupamentos, enquanto o NNI nos d√° uma **medida geral** do grau de concentra√ß√£o de toda a sua opera√ß√£o.
                """)
                st.subheader("Perguntas Frequentes (FAQ)")
                st.markdown("""
                #### O agrupamento dos servi√ßos √© definido por "c√≠rculos"? Os pontos de um "c√≠rculo" invadem o outro? Como fica a regi√£o aglomerada por 4 "c√≠rculos"? N√£o fica um espa√ßo n√£o mapeado no meio?
                
                Essa √© uma √≥tima pergunta! Ao contr√°rio do que se pode imaginar, o algoritmo DBSCAN n√£o desenha c√≠rculos fixos e independentes no mapa. Ele funciona mais como uma "mancha de tinta que se espalha" para identificar as √°reas densas.
                
                Pense assim:
                1.  O DBSCAN come√ßa em um ponto.
                2.  Ele verifica se h√° vizinhos suficientes dentro de um **raio** espec√≠fico (o "Raio do Cluster (km)" que voc√™ ajusta).
                3.  Se houver, ele considera esse ponto parte de um cluster e **se expande** para incluir todos os vizinhos densos, e os vizinhos desses vizinhos, e assim por diante.
                
                Isso significa que:
                - **N√£o s√£o c√≠rculos r√≠gidos:** Os clusters resultantes t√™m **formas irregulares e org√¢nicas**, adaptando-se √† distribui√ß√£o real dos seus dados (por exemplo, seguindo o tra√ßado de uma rua ou o contorno de um bairro).
                - **Os agrupamentos se fundem:** Se as "√°reas de influ√™ncia" de pontos pr√≥ximos se sobrep√µem e ambos s√£o densos, eles se tornam parte do **mesmo cluster grande**. N√£o h√° "invas√£o" de c√≠rculos, mas sim uma fus√£o natural.
                - **N√£o ficam espa√ßos n√£o mapeados no meio:** Em uma regi√£o aglomerada por v√°rios pontos densos, o DBSCAN n√£o deixa buracos. Ele forma um √∫nico cluster cont√≠nuo que cobre toda a √°rea densamente populada por servi√ßos. O resultado √© uma representa√ß√£o muito mais fiel das suas "zonas de trabalho" do que simples c√≠rculos.
                
                #### Por que o DBSCAN √© mais adequado para esta ferramenta do que "mapear por km¬≤" ou "mapas de calor"?
                
                Sua sugest√£o de "mapear por km¬≤" √© excelente e se aproxima muito de uma t√©cnica conhecida como **An√°lise de Grade** ou **Mapa de Calor Hexagonal** (que adicionamos em uma nova aba!).
                
                Ambas as abordagens s√£o valiosas, mas com focos diferentes:
                - **DBSCAN (Clusters Irregulares):** Ideal para **otimiza√ß√£o log√≠stica**. Os clusters que ele identifica representam as **"zonas de trabalho naturais"** da sua opera√ß√£o, onde uma equipe pode atender m√∫ltiplos servi√ßos com m√≠nimo deslocamento. Ele √© focado em *agrupamentos reais de servi√ßos*.
                - **Mapa Hexagonal (Visualiza√ß√£o de Densidade em Grade):** Perfeito para **percep√ß√£o r√°pida de densidade** e relat√≥rios gerenciais. Ele mostra visualmente onde h√° maior concentra√ß√£o de pontos em √°reas geogr√°ficas fixas, independentemente de formarem clusters estatisticamente significativos. √â mais focado em *onde est√° mais "quente" de servi√ßos*.
                
                Para a otimiza√ß√£o de rotas e aloca√ß√£o de equipes, os clusters org√¢nicos do DBSCAN s√£o geralmente mais √∫teis porque eles delimitam √°reas de forma mais inteligente para o campo. O mapa hexagonal, por sua vez, complementa essa vis√£o, mostrando as "manchas" gerais de atividade. Juntos, eles oferecem uma an√°lise completa!
                """)
        else:
            st.warning("Nenhum dado para exibir com os filtros atuais.")
else:
    st.info("Aguardando o upload de um arquivo para iniciar a an√°lise.")
# ==============================================================================
# 1. IMPORTA√á√ÉO DAS BIBLIOTECAS
# ==============================================================================
import streamlit as st
import pandas as pd
import geopandas as gpd
import numpy as np
from sklearn.cluster import DBSCAN
from scipy.spatial import distance
from math import sqrt
import folium
from streamlit_folium import st_folium

# ==============================================================================
# 2. CONFIGURA√á√ÉO DA P√ÅGINA E T√çTULOS
# ==============================================================================
st.set_page_config(layout="wide", page_title="An√°lise de Dispers√£o Geogr√°fica")

st.title("üó∫Ô∏è Ferramenta de An√°lise de Dispers√£o Geogr√°fica")
st.write("""
    Fa√ßa o upload da sua planilha de cortes para analisar a distribui√ß√£o geogr√°fica,
    identificar clusters e obter insights para sua opera√ß√£o log√≠stica.
""")

# ==============================================================================
# 3. FUN√á√ïES DE AN√ÅLISE (COM CACHE PARA PERFORMANCE)
# ==============================================================================

@st.cache_data
def carregar_dados(arquivo_enviado):
    """L√™ o arquivo CSV ou Excel, tentando diferentes codifica√ß√µes e separadores."""
    # Lista de codifica√ß√µes comuns para tentar
    encodings_to_try = ['utf-8-sig', 'latin-1', 'utf-8', 'iso-8859-1']
    
    # Tenta ler como CSV com diferentes codifica√ß√µes
    for encoding in encodings_to_try:
        try:
            arquivo_enviado.seek(0)
            df = pd.read_csv(arquivo_enviado, encoding=encoding, sep=None, engine='python')
            st.success(f"Arquivo CSV lido com sucesso usando a codifica√ß√£o: {encoding}")
            
            if 'latitude' not in df.columns or 'longitude' not in df.columns:
                st.error("O arquivo precisa conter as colunas 'latitude' e 'longitude'.")
                return None

            df['latitude'] = df['latitude'].astype(str).str.replace(',', '.').astype(float)
            df['longitude'] = df['longitude'].astype(str).str.replace(',', '.').astype(float)
            df = df.dropna(subset=['latitude', 'longitude'])
            return df
        except Exception:
            continue

    # Se todas as tentativas de CSV falharem, tenta ler como Excel
    try:
        arquivo_enviado.seek(0)
        df = pd.read_excel(arquivo_enviado, engine='openpyxl')
        st.success("Arquivo lido com sucesso como Excel (.xlsx).")

        if 'latitude' not in df.columns or 'longitude' not in df.columns:
            st.error("O arquivo precisa conter as colunas 'latitude' e 'longitude'.")
            return None
        df['latitude'] = df['latitude'].astype(str).str.replace(',', '.').astype(float)
        df['longitude'] = df['longitude'].astype(str).str.replace(',', '.').astype(float)
        df = df.dropna(subset=['latitude', 'longitude'])
        return df
    except Exception as e:
        st.error(f"N√£o foi poss√≠vel ler o arquivo como CSV ou Excel. √öltimo erro: {e}")
        return None

def calcular_nni(gdf):
    """Calcula o √çndice do Vizinho Mais Pr√≥ximo (NNI)."""
    if len(gdf) < 3:
        return None, "Pontos insuficientes (< 3) para calcular NNI."

    n_points = len(gdf)
    points = np.array([gdf.geometry.x, gdf.geometry.y]).T
    
    try:
        total_bounds = gdf.total_bounds
        area = (total_bounds[2] - total_bounds[0]) * (total_bounds[3] - total_bounds[1])
        if area == 0:
            return None, "√Årea inv√°lida (pontos colineares)."

        dist_matrix = distance.cdist(points, points, 'euclidean')
        np.fill_diagonal(dist_matrix, np.inf)
        
        nearest_neighbor_dists = dist_matrix.min(axis=1)
        observed_mean_dist = nearest_neighbor_dists.mean()
        
        expected_mean_dist = 0.5 * sqrt(area / n_points)
        
        nni = observed_mean_dist / expected_mean_dist
        
        if nni < 1:
            interpretacao = f"Agrupado (NNI: {nni:.2f})"
        elif nni > 1:
            interpretacao = f"Disperso (NNI: {nni:.2f})"
        else:
            interpretacao = f"Aleat√≥rio (NNI: {nni:.2f})"
        
        return nni, interpretacao
    except Exception as e:
        return None, f"Erro no c√°lculo: {e}"

def executar_dbscan(gdf, eps_km=0.5, min_samples=3):
    """Executa o DBSCAN para encontrar clusters."""
    if gdf.empty or len(gdf) < min_samples:
        gdf['cluster'] = -1
        return gdf
    
    raio_terra_km = 6371
    eps_rad = eps_km / raio_terra_km
    
    coords = np.radians(gdf[['latitude', 'longitude']].values)
    db = DBSCAN(eps=eps_rad, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(coords)
    gdf['cluster'] = db.labels_
    return gdf

# ==============================================================================
# 4. BARRA LATERAL (SIDEBAR) COM UPLOAD E FILTROS
# ==============================================================================
st.sidebar.header("Controles")
uploaded_file = st.sidebar.file_uploader(
    "Escolha a planilha de cortes",
    type=["csv", "xlsx", "xls"]
)

if uploaded_file is not None:
    df_completo = carregar_dados(uploaded_file)

    if df_completo is not None:
        st.sidebar.success(f"{len(df_completo)} registros carregados!")
        
        st.sidebar.markdown("### Filtros da An√°lise")
        
        filtros = ['sucursal', 'Centro_Operativo', 'corte_recorte', 'Prioridade']
        valores_selecionados = {}

        for coluna in filtros:
            if coluna in df_completo.columns:
                opcoes = ["Todos"] + sorted(df_completo[coluna].dropna().unique().tolist())
                valores_selecionados[coluna] = st.sidebar.selectbox(f"{coluna.replace('_', ' ').title()}", opcoes)

        df_filtrado = df_completo.copy()
        for coluna, valor in valores_selecionados.items():
            if valor != "Todos":
                df_filtrado = df_filtrado[df_filtrado[coluna] == valor]

        # ==============================================================================
        # 5. √ÅREA PRINCIPAL COM RESULTADOS E MAPA
        # ==============================================================================
        
        st.header("Resultados da An√°lise")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Total de Cortes no Arquivo", len(df_completo))
        col2.metric("Cortes na Sele√ß√£o Atual", len(df_filtrado))
        
        gdf_filtrado = gpd.GeoDataFrame(
            df_filtrado,
            geometry=gpd.points_from_xy(df_filtrado.longitude, df_filtrado.latitude),
            crs="EPSG:4326"
        )
        
        nni_valor, nni_texto = calcular_nni(gdf_filtrado)
        col3.metric("Padr√£o de Dispers√£o", nni_texto)
        
        st.sidebar.markdown("### Par√¢metros de Cluster")
        eps_cluster_km = st.sidebar.slider("Raio do Cluster (km)", 0.1, 5.0, 1.0, 0.1)
        min_samples_cluster = st.sidebar.slider("M√≠nimo de Pontos por Cluster", 2, 20, 5, 1)

        gdf_com_clusters = executar_dbscan(gdf_filtrado, eps_km=eps_cluster_km, min_samples=min_samples_cluster)
        
        n_clusters = len(set(gdf_com_clusters['cluster'])) - (1 if -1 in gdf_com_clusters['cluster'] else 0)
        n_ruido = list(gdf_com_clusters['cluster']).count(-1)

        st.subheader(f"An√°lise de Clusters: {n_clusters} hotspots encontrados")
        st.write(f"Foram encontrados **{n_clusters} clusters** e **{n_ruido} pontos isolados** com os par√¢metros definidos.")
        
        if not gdf_com_clusters.empty:
            map_center = [gdf_com_clusters.latitude.mean(), gdf_com_clusters.longitude.mean()]
            m = folium.Map(location=map_center, zoom_start=11)

            unique_clusters = sorted(gdf_com_clusters['cluster'].unique())
            colors = ['#%06X' % np.random.randint(0, 0xFFFFFF) for i in range(len(unique_clusters))]
            cluster_colors = {cluster_id: color for cluster_id, color in zip(unique_clusters, colors)}
            
            for idx, row in gdf_com_clusters.iterrows():
                cluster_id = row['cluster']
                color = cluster_colors.get(cluster_id, '#000000') if cluster_id != -1 else '#000000'
                
                popup_text = ""
                for col in ['Prioridade', 'Centro_Operativo', 'corte_recorte']:
                    if col in row:
                        popup_text += f"{col.replace('_', ' ').title()}: {row[col]}<br>"

                folium.CircleMarker(
                    location=[row['latitude'], row['longitude']],
                    radius=5,
                    color=color,
                    fill=True,
                    fill_color=color,
                    fill_opacity=0.7,
                    popup=popup_text
                ).add_to(m)
            
            st_folium(m, width=725, height=500, returned_objects=[])
        else:
            st.warning("Nenhum dado para exibir no mapa com os filtros atuais.")

else:
    st.info("Aguardando o upload de um arquivo para iniciar a an√°lise.")
